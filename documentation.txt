1. Cloned ethash and geth and played around with the source
2. Saw that ethash also has a nodejs implementation of a lightweight client.
3. Ported that over to vanilla javascript
4. Demo (around 1500 hashes per second)
5. Looked how the DAG and Cache are generated in both ethash and Geth.
6. So the Dag is generated at /geth/consensus/ethash/algorithm.go
   func generateDataset(dest []uint32, epoch uint64, cache [] uint32)
7. So the Dag seems to be an array, whose length is dependent on the block number.
8. I printed out the DAG and the cache and got arrays.
9. I also inserted print statements to get the size of the cache and the dag
10. Used Geth to ./geth makedag 4983011 ~/.ethash, we get the following for the current block number:
    Cache size: 38535104
    DAG size: 2466247808 (2.4 GB)
11. Also found the exact place in geth where the mining happens , and printed out all the arguments.
12. I could look into this more and serialize all these arguments, send it over the network, have the browser mine, and send the results back to geth. We could then make this work with an actual ethereum miner implementation.4

Calculations on how many machines needed and for the other method as well, calculations.



Running the testing scripts (/test/test.sh) for the offical ethash source tree

------------------------------------------------CACHE-----------------------------------------

There  exists  a  seed  which  can  be  computed  for  each
block by scanning through the block headers up until that
point.  From the seed, one can compute a pseudorandom
cache, X in  initial  size. 

Ethash Javascript:

Cache:
Uint32Array {
  '0': 1442248915,
  '1': 2295887220,
  '2': 1123104972,
  '3': 1621115537,
  '4': 641816488,
  '5': 152286957,
  '6': 3647008846,
  '7': 1685058579,
  '8': 3121943381,
  '9': 614392864,
  '10': 1608061077,
  ...
  '262085': 2429168457,
  '262086': 1462939248,
  '262087': 3985340622,
  '262088': 2087348941,
  '262089': 99943832,
  '262090': 3229472126,
  '262091': 3027677228,
  '262092': 499290970,
  '262093': 4048164279,
  '262094': 3513695759,
  '262095': 1187373974 }

Ethash C :

Used ethash_get_cachesize in /src/libethash/internal.c to Cache size in bytes:
16907456 (div 64 = 262139)
17039296 (div 64 = 266239)
17170112 

---- from geth ----
I put a print statement for the cachesize and noted that the longer I let geth run,
the cache size increased:

16776896
16907456
17301056
17693888
17824192
18481088
19267264
19529408


Note that if we divide these numbers by 64, we get numbers similar to the length of the javascript array

--------------------------------------------------HASHING----------------------------------------------

The main hashing in C is done by ethash_hash(), which has the following snippet

for (unsigned n = 0; n != MIX_NODES; ++n) {
      node const* dag_node;
      if (full_nodes) {
        dag_node = &full_nodes[MIX_NODES * index + n];
      } else {
        node tmp_node;
        ethash_calculate_dag_item(&tmp_node, index * MIX_NODES + n, light);
        dag_node = &tmp_node;
      }

This snippet checks for a full node vs a light-weight client. If it is a light weight client, it proceeds to calculate the dag item vs for the full node, it just directly accesses the dag item from the dag itself.

----------------------------------------------DAG-----------------------------------------------------

Each item of the DAG is calculated by :

void ethash_calculate_dag_item(
  node* const ret,
  uint32_t node_index,
  ethash_light_t const light
)

This is located in /src/libethash/internal.c

Hence, each DAG item is a node. A node looks like the following (defined in src/libethash/internal.h):

typedef union node {
  uint8_t bytes[NODE_WORDS * 4];
  uint32_t words[NODE_WORDS];
  uint64_t double_words[NODE_WORDS / 2];

#if defined(_M_X64) && ENABLE_SSE
  __m128i xmm[NODE_WORDS/4];
#endif

} node;

from the ethash wiki:

    There exists a seed which can be computed for each block by scanning through the block headers up until that point.
    From the seed, one can compute a 16 MB pseudorandom cache. Light clients store the cache.
    From the cache, we can generate a 1 GB dataset, with the property that each item in the dataset depends on only a small number of items from the cache. Full clients and miners store the dataset. The dataset grows linearly with time.
    Mining involves grabbing random slices of the dataset and hashing them together. Verification can be done with low memory by using the cache to regenerate the specific pieces of the dataset that you need, so you only need to store the cache.

Q: What are the cache and the DAG dependent on?

Really good resources:
https://ethereum.stackexchange.com/questions/31599/is-there-any-documentation-to-visualize-example-of-the-original-dag-graph-that-e

---------------------------------------------------------------------------------
https://www.vijaypradeep.com/blog/2017-04-28-ethereums-memory-hardness-explained/
---------------------------------------------------------------------------------
https://github.com/ethereum/wiki/wiki/Mining#ethash-dag

DAG src 1:
The Ethash algorithm relies on a pseudorandom dataset, initialized by the current blockchain length.  This is called a DAG, and is regenerated every 30,000 blocks (or every ~5 days).  As of March 2017, the DAG was ~2GB [2], and the DAG will continue grow in size as the blockchain grows.

DAG src 2:
Ethash uses a DAG (directed acyclic graph) for the proof of work algorithm, this is generated for each epoch, i.e every 30000 blocks (100 hours). The DAG takes a long time to generate. If clients only generate it on demand, you may see a long wait at each epoch transition before the first block of the new epoch is found. However, the DAG only depends on block number, so it CAN and SHOULD be calculated in advance to avoid long wait at each epoch transition. geth implements automatic DAG generation and maintains two DAGS at a time for smooth epoch transitions. Automatic DAG generation is turned on and off when mining is controlled from the console. It is also turned on by default if geth is launched with the --mine option. Note that clients share a DAG resource, so if you are running multiple instances of any client, make sure automatic dag generation is switched on in at most one client.

Cache and Dag

Geth -----
THE ACTUAL FUNCTION THAT IS CALLED TO GENERATE THE DATASET IS AS FOLLOWS:
THIS IS LOCATED IN /geth/consensus/ethash/algorithm.go

func generateDataset(dest []uint32, epoch uint64, cache [] uint32)
// generateDataset generates the entire ethash dataset for mining.
// This method places the result into dest in machine byte order.


  I patched geth to include 2 print statements for the cache size and the DAG size in the function generate()

  Used : ./geth makedag <block-number> <output dir>
  and ran the following to generate the DAG
        ./geth makedag 360000 ~/.ethash
This is what I got for the DAG size and Cache size:
  Cache size: 18349504
  DAG size: 1174404736

Then I ran it with the current block number (as of Sat 27 Jan 11:54 AM)
./geth makedag 4983011 ~/.ethash
  Cache size: 38535104
  DAG size: 2466247808 (2.4 GB)


What is the cache:
an array of ints
what is the dag:
an array of ints

--------------------------------------------extras--------------------------------------
There are 3 places where the cache and dag seem to be generated:

1) Cache:

  newCache(epoch)
  // newCache creates a new ethash verification cache and returns it as a plain Go
  // interface to be usable in an LRU cache.
  
  generate(dir string, limit int, test bool) 
  dir = to store cache on disk, limit = used to delete old caches that are no longer needed from disk, test = 0 or 1 depending on whether this is a test env
  // generate ensures that the cache content is generated before use.

  MakeCache(block uint64, dir string)
  // MakeCache generates a new ethash cache and optionally stores it to disk.

2) Dataset:
  newDataset(epoch)
  generate(dir string, limit int, test bool) 
  MakeDataset(block uint64, dir string)

could not find a cache object anywhere (but did find a node object defined in 
/src/libethash/internal.h)
